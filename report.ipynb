{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Smartcab to Drive\n",
    "## Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Basic Driving Agent\n",
    "To begin, your only task is to get the smartcab to move around in the environment. \n",
    "At this point, you will not be concerned with any sort of optimal driving policy. \n",
    "Note that the driving agent is given the following information at each intersection:\n",
    "\n",
    "- The next waypoint location relative to its current location and heading.\n",
    "- The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "- The current time left from the allotted deadline.\n",
    "\n",
    "To complete this task, simply have your driving agent choose a random action \n",
    "from the set of possible actions (None, 'forward', 'left', 'right') at each intersection, disregarding the input information above. \n",
    "Set the simulation deadline enforcement, enforce_deadline to False and observe how it performs.\n",
    "\n",
    "**QUESTION:** \n",
    "\n",
    "Observe what you see with the agent's behavior as it takes random actions. \n",
    "Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The angent's behaviour is, unsurprisingly, random. It would at times move in a circle (around the block) not really progressing in any net new direction. And it doesn't appear to have any logic behind its movement as rewards for previous states and actions are ignored.\n",
    "Another item of the agent's behavior is the agent can also drive over the right edge of the world and turn up on the left edge of the world.\n",
    "However, the smartcab does eventually make it to its destination on occasion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inform the Driving Agent\n",
    "Now that your driving agent is capable of moving around in the environment, \n",
    "your next task is to identify a set of states that are appropriate for modeling the smartcab and environment. \n",
    "The main source of state variables are the current inputs at the intersection, but not all may require representation. \n",
    "You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, \n",
    "process the inputs and update the agent's current state using the self.state variable. \n",
    "\n",
    "Continue with the simulation deadline enforcement enforce_deadline being set to False, \n",
    "and observe how your driving agent now reports the change in state as the simulation progresses.\n",
    "\n",
    "**QUESTION:** \n",
    "\n",
    "What states have you identified that are appropriate for modeling the smartcab and environment? \n",
    "Why do you believe each of these states to be appropriate for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "At each state or waypoint we perceive 6 inputs, left, right, oncoming, light, next_waypoint and deadline.\n",
    "Since traffic coming from the right, doesn't affect the smartcab's decision, this variable was removed - e.g. if the \n",
    "light is green, traffic from the right has stopped and can be ignored. Likewise, if light is red, the smartcab can only turn right and any traffic coming from the right doesn't affect the agent's options. The input deadline was not utilized because at this point it does not provide any significant information to the smartcab (enforce_deadline set to false).\n",
    "\n",
    "Each state identifies a situation and option for the smartcab, which results in a particular reward. Once Q-learning is implemented (below) the smartcab will be able to use these states to learn and best determine its path to the destination.\n",
    " \n",
    "Final states used are light, oncoming, left and next_waypoint, each of which are appropriate for this problem. \n",
    "\n",
    "**OPTIONAL:** \n",
    "\n",
    "How many states in total exist for the smartcab in this environment? \n",
    "Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Inputs and their possible values:\n",
    "\n",
    "- Light: Red or green\n",
    "- Oncoming: None, left, right, forward\n",
    "- Left: None, left, right, forward\n",
    "- Next_waypoint: left, right, forward\n",
    "\n",
    "Capturing 4 inputs per state with each input having a variation between 2-4, gives us, 2x4x4x3, 96 maximum number of states in total that exist for the smartcab in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Q-Learning Driving Agent\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, \n",
    "your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, \n",
    "based on the Q-values for the current state and action. \n",
    "\n",
    "Each action taken by the smartcab will produce a reward which depends on the state of the environment. \n",
    "The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, \n",
    "set the simulation deadline enforcement enforce_deadline to True. Run the simulation and observe how the smartcab moves about the environment in each trial.\n",
    "\n",
    "**QUESTION:**\n",
    "\n",
    "What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? \n",
    "Why is this behavior occurring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "At the beginning of trials the agent's behavior remains the same; however, with larger \"n_trials\" (more trials), the agent no longer seems to act randomly but its movement tends towards the destination. It also appears more \"structured\" or intentional instead of moving randomly as time progresses.\n",
    "\n",
    "The agent also reaching its destination more often and seemingly quicker, even though the deadline was disabled during the random trials. \n",
    "\n",
    "Metrics used to track the agent's performance are: \n",
    "\n",
    "- Reward: Total reward, sum of positive and negative rewards received at each state\n",
    "- Win: The number of times the agent reaches its destination (deadline is enforced)\n",
    "- Lose: The number of times the agent runs out of time without reaching its destination (deadline is enforced)\n",
    "- Penalties: Total negative reward, sum of negative rewards received\n",
    "\n",
    "This behvaior occurs because it is learning from its past experience. The q-table starts off empty and as the agent explores its world, the q-table is being updated based on the outcome of each state, action and reward. Then, if the agent comes across a similiar state (already stored in the q-table) the agent will utilize this knowledge, helping the agent make \"better\" decisions that maximize reward.\n",
    "\n",
    "The tables below show the results of 10 Runs of 50 Trials (n_trials = 50) each, for both random actions and those of the implemented Q-learning algorithmn. \n",
    "\n",
    "**Random actions:** Average Rewards: 7.6, Win: 11, Lose: 39, Penalties: -568\n",
    "\n",
    "Rewards | Win | Lose | Penalties\n",
    "--- | --- | --- | ---\n",
    "49.0 | 11 | 39 | -589.5 \n",
    "-34.5 | 13 | 37 | -593.0\n",
    "-31 | 7 | 43 | -558.0\n",
    "49.5 | 14 | 36 | -512.5\n",
    "5 | 10 | 40 | -587.0\n",
    "\n",
    "**Q-Learning:** Average Rewards: 1059, Win: 40, Lose: 10, Penalties: -164\n",
    "\n",
    "Rewards | Win | Lose | Penalties\n",
    "--- | --- | --- | ---\n",
    "887.5 | 36 | 14 | -155.0\n",
    "1016.0 | 41 | 9 | -195.5\n",
    "1154.5 | 47 | 3 | -137.5\n",
    "1097.0 | 36 | 14 | -145.0\n",
    "1140.5 | 42 | 8 | -185.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Q-Learning Driving Agent\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, \n",
    "the smartcab is able to reach the destination within the allotted time safely and efficiently. \n",
    "Parameters in the Q-Learning algorithm, such as the learning rate (alpha), \n",
    "the discount factor (gamma) and the exploration rate (epsilon) all contribute to the driving agent’s ability to learn the best action for each state. \n",
    "To improve on the success of your smartcab:\n",
    "\n",
    "- Set the number of trials, n_trials, in the simulation to 100.\n",
    "- Run the simulation with the deadline enforcement enforce_deadline set to True (you will need to reduce the update delay update_delay and set the display to False).\n",
    "- Observe the driving agent’s learning and smartcab’s success rate, particularly during the later trials.\n",
    "- Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully.\n",
    "\n",
    "**QUESTION:** \n",
    "\n",
    "Report the different values for the parameters tuned in your basic implementation of Q-Learning. \n",
    "For which set of parameters does the agent perform best? How well does the final driving agent perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Mainly 3 parameters were tuned in order to find the best combination where the agent performs best, these parameters are and how they impact the agent is described below:\n",
    "\n",
    "- Alpha (learning rate), epsilon (exploration rate) and gamma (discount factor) were utilized.\n",
    "- If alpha is too small, learning takes too long, too large and learning will put\n",
    "too much importance on the most recent action\n",
    "- If gamma is too small, too much importance is placed on the current reward, too large and \n",
    "emphasis is placed on the future reward\n",
    "\n",
    "Tuning was also performed using a technique called optimistic initialization, by updating the q-table with non-zero values higher than our first reward of 2.0, causing the agent to perform more exploration at the beginning of the trials.\n",
    "\n",
    "The agent performed best with the following set of parameters: alpha = 0.6, epsilon = 0.1, gamma = 0.1. At it's most optimal, the driving agent fails to reach its destination only once out of its 100 trials. \n",
    "\n",
    "Tables below are 3 samples from multiple runs (5 runs, 100 trials) experimenting with alpha, epsilon and gamma values:\n",
    "\n",
    "**alpha = 0.2, epsilon = 0.1, gamma = 0.4**\n",
    "\n",
    "Rewards | Win | Lose | Penalties\n",
    "--- | --- | --- | ---\n",
    "2231.5 | 87 | 13 | -255.5\n",
    "2226.0 | 86 | 14 | -271.5\n",
    "2083.5 | 83 | 17 | -230.0\n",
    "2068.0 | 93 | 7 | -265.0\n",
    "2056.5 | 82 | 18 | -316.5\n",
    "\n",
    "**alpha = 0.4, epsilon = 0.3, gamma = 0.1**\n",
    "\n",
    "Rewards | Win | Lose | Penalties\n",
    "--- | --- | --- | ---\n",
    "2033.5 | 82 | 18 | -402.0\n",
    "2059.5 | 81 | 19 | -408.0\n",
    "1989.5 | 75 | 25 | -369.5\n",
    "2031.0 | 81 | 19 | -396.5\n",
    "1991.0 | 79 | 21 | -380.5\n",
    "\n",
    "**alpha = 0.6, epsilon = 0.1, gamma = 0.1**\n",
    "\n",
    "Rewards | Win | Lose | Penalties\n",
    "--- | --- | --- | ---\n",
    "2253.0 | 95 | 5 | -119.0\n",
    "2300.5 | 96 | 4 | -115.0\n",
    "2180.5 | 94 | 6 | -145.5\n",
    "2233.5 | 95 | 5 | -108.5\n",
    "2275.5 | 99 | 1 | -96.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:**\n",
    "\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, \n",
    "and not incur any penalties? How would you describe an optimal policy for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optimal policy: Always get to your destination without incurring penalties (illegal actions e.g. driving through a red light) and do it in the shortest amount of time.\n",
    "\n",
    "The optimal / learned policy for this problem is realitively balanced between gaining maximium reward and always arriving at it's destination but it does incur penalities to do so. However, the learned policy does have a much lower rate of incurred penalities during higher number of trials.\n",
    "\n",
    "Based on the above, and the previously supplied table data, it can be concluded that the learned policy does come very close to an optimal policy, particularly if you're willing to incur some penalties. But given that the agent is simulating a smartcab and penalties are illegal traffic actions, it would still be unacceptable for a real world scenario. In which case less emphasis on maximizing rewards while still reaching its destination would be ideal."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
